{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7TpVKeITKFY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import os\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, jensen_shannon\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print('Downloads Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbTRSwxgTvUC"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def initial_clean(text):\n",
    "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = text.lower() # lower case the text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "def pos(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "informative_pos = ('JJ','VB', 'NN','RBS','VBP','IN','RBR','JJR','JJS','PDT','RP','UH','FW','NNS','VBN','VBG')\n",
    "\n",
    "def remove_uninformative_pos(text):\n",
    "    tagged_words = nltk.pos_tag(text)\n",
    "    return [word for word, tag in tagged_words if tag in informative_pos]\n",
    "  \n",
    "clutter = ['food','place','good','order','great','like',\n",
    "           'service','time','go','ordered','get','love',\n",
    "           'best','come','eat','dont','tried','try','ask',\n",
    "           'nice','restaurant','ive','im','didnt']\n",
    "\n",
    "def remove_garbage(text):\n",
    "    return [word for word in text if word not in clutter]\n",
    "\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
    "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(remove_garbage(remove_uninformative_pos(remove_stop_words(initial_clean(text)))))\n",
    "\n",
    "def get_top_k_words(df, k = 10000):\n",
    "    # first get a list of all words\n",
    "    all_words = [word for item in list(df['tokenized']) for word in item]\n",
    "    \n",
    "    # use nltk fdist to get a frequency distribution of all words\n",
    "    fdist = FreqDist(all_words)\n",
    "    \n",
    "    # define a function only to keep words in the top k words\n",
    "    top_k_words, _ = zip(*fdist.most_common(k))\n",
    "    top_k_words = set(top_k_words)\n",
    "    \n",
    "    return top_k_words\n",
    "\n",
    "def keep_top_k_words(text, *top_k_words):\n",
    "    return [word for word in text if word in top_k_words]\n",
    "\n",
    "def transform_dataset(df):\n",
    "    # format the columns\n",
    "    df = df.groupby(['business_id', 'name'])['text'].apply(' '.join).reset_index()\n",
    "    df = df[df['text'].map(type) == str]\n",
    "    df.dropna(axis=0, inplace=True, subset=['text'])\n",
    "    return df\n",
    "\n",
    "def gen_tokenized_column(df):\n",
    "    # preprocess the text and business name and create new column \"tokenized\"\n",
    "    df['tokenized'] = df['text'].apply(apply_all)\n",
    "    top_k_words = get_top_k_words(df)\n",
    "    df['tokenized'] = df['tokenized'].apply(keep_top_k_words, args=(top_k_words))\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(df):\n",
    "    t1 = time.time()\n",
    "    preprocessed_df = gen_tokenized_column(transform_dataset(df))\n",
    "    t2 = time.time()\n",
    "    print(\"Time to clean and tokenize\", len(df), \"businesses' reviews:\", (t2-t1)/60, \"min\")\n",
    "    return preprocessed_df\n",
    "    \n",
    "def get_coherence_score(model, texts, dictionary):\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "def get_dictionary_corpus(data, no_below=20, no_above=0.5):\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def get_perplexity(model, corpus):\n",
    "    # a measure of how good the model is; lower the better\n",
    "    return lda_model.log_perplexity(corpus)\n",
    "\n",
    "def train_lda(corpus, id2word, chunksize=2000, num_topics=12, alpha='auto', eta='auto', passes=1, iterations=50,\n",
    "              minimum_probability=0.01, eval_every=10, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, alpha=alpha, eta=eta, \n",
    "                   chunksize=chunksize, minimum_probability=minimum_probability, passes=passes, \n",
    "                   iterations=iterations, eval_every=eval_every, random_state=random_state)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lda\n",
    "\n",
    "def train_hdp(corpus, id2word, chunksize=2000, T=150, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the hdp model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    hdp = HdpModel(corpus=corpus, id2word=id2word, T=T, chunksize=chunksize, random_state=random_state)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train HDP model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return hdp\n",
    "\n",
    "def train_lsi(corpus, id2word, num_topics=12, chunksize=2000, onepass=True, power_iters=2, extra_samples=100):\n",
    "    \"\"\"\n",
    "    This function trains the lsi model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lsi = LsiModel(corpus=corpus, num_topics=num_topics, id2word=id2word, chunksize=chunksize)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LSI model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lsi\n",
    "\n",
    "def get_most_similar_documents(query, corpus, dictionary, k=10):\n",
    "    distances = []\n",
    "    for c in corpus:\n",
    "        distances.append(kullback_leibler(query, c, num_features=len(dictionary)))\n",
    "    \n",
    "    indices = np.array(distances).argsort()[:k]\n",
    "    return indices\n",
    "\n",
    "def get_topic_dist(model, corpus):\n",
    "    doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in model[corpus]])\n",
    "    return doc_topic_dist\n",
    "\n",
    "def get_most_similar_businesses(query_data, corpus, dictionary, model):\n",
    "    query_bow = dictionary.doc2bow(query_data)\n",
    "    most_sim_ids = get_most_similar_documents(model[query_bow], model[corpus], dictionary)\n",
    "    return most_sim_ids \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LDA\n",
    "def select_num_topics_LDA(dictionary, corpus, texts, end, start=4, step=4):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lda(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_num_topics_HDP(dictionary, corpus, texts, end, start=4, step=4):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the HDP model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, end + 1, step): \n",
    "        model = train_hdp(corpus=corpus, id2word=dictionary, T=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "            \n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "    \n",
    "    return model, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LSI\n",
    "def select_num_topics_LSI(dictionary, corpus, texts, end, start=4, step=4):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LSI topic models\n",
    "    coherence_values : Coherence values corresponding to the Lsi model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing mesa_5000.csv finished\n",
      "Time to train LSI model on businesses:  0.00359111229578654 min\n",
      "progress: num of topics:  3\n",
      "Time to train LSI model on businesses:  0.004649563630421957 min\n",
      "progress: num of topics:  6\n",
      "Time to train LSI model on businesses:  0.00479360818862915 min\n",
      "progress: num of topics:  9\n",
      "Time to train LSI model on businesses:  0.004728670914967855 min\n",
      "progress: num of topics:  12\n",
      "Time to train LSI model on businesses:  0.004720441500345866 min\n",
      "progress: num of topics:  15\n",
      "Time to train LSI model on businesses:  0.00493397315343221 min\n",
      "progress: num of topics:  18\n",
      "Time to train LSI model on businesses:  0.005238668123881022 min\n",
      "progress: num of topics:  21\n",
      "Time to train LSI model on businesses:  0.005575935045878093 min\n",
      "progress: num of topics:  24\n",
      "Time to train LSI model on businesses:  0.0053895354270935055 min\n",
      "progress: num of topics:  27\n",
      "Time to train LSI model on businesses:  0.004775222142537435 min\n",
      "progress: num of topics:  30\n",
      "Time to train LSI model on businesses:  0.005218231678009033 min\n",
      "progress: num of topics:  33\n",
      "Time to train LSI model on businesses:  0.0052826245625813804 min\n",
      "progress: num of topics:  36\n",
      "Time to train LSI model on businesses:  0.005829540888468424 min\n",
      "progress: num of topics:  39\n",
      "Time to train LDA model on businesses:  0.025981183846791586 min\n",
      "progress: num of topics:  3\n",
      "Time to train LDA model on businesses:  0.03333719571431478 min\n",
      "progress: num of topics:  6\n",
      "Time to train LDA model on businesses:  0.04615182876586914 min\n",
      "progress: num of topics:  9\n",
      "Time to train LDA model on businesses:  0.06293407281239828 min\n",
      "progress: num of topics:  12\n",
      "Time to train LDA model on businesses:  0.046584657828013104 min\n",
      "progress: num of topics:  15\n",
      "Time to train LDA model on businesses:  0.0498177170753479 min\n",
      "progress: num of topics:  18\n",
      "Time to train LDA model on businesses:  0.06979549725850423 min\n",
      "progress: num of topics:  21\n",
      "Time to train LDA model on businesses:  0.0751298467318217 min\n",
      "progress: num of topics:  24\n",
      "Time to train LDA model on businesses:  0.07234448194503784 min\n",
      "progress: num of topics:  27\n",
      "Time to train LDA model on businesses:  0.06876310507456461 min\n",
      "progress: num of topics:  30\n",
      "Time to train LDA model on businesses:  0.06772336959838868 min\n",
      "progress: num of topics:  33\n",
      "Time to train LDA model on businesses:  0.07319344679514567 min\n",
      "progress: num of topics:  36\n",
      "Time to train LDA model on businesses:  0.08042937914530436 min\n",
      "progress: num of topics:  39\n",
      "Time to train HDP model on businesses:  0.013349556922912597 min\n",
      "progress: num of topics:  3\n",
      "Time to train HDP model on businesses:  0.01425466537475586 min\n",
      "progress: num of topics:  6\n",
      "Time to train HDP model on businesses:  0.015120943387349447 min\n",
      "progress: num of topics:  9\n",
      "Time to train HDP model on businesses:  0.017226266860961913 min\n",
      "progress: num of topics:  12\n",
      "Time to train HDP model on businesses:  0.020539478460947672 min\n",
      "progress: num of topics:  15\n",
      "Time to train HDP model on businesses:  0.02534992297490438 min\n",
      "progress: num of topics:  18\n",
      "Time to train HDP model on businesses:  0.020505825678507488 min\n",
      "progress: num of topics:  21\n",
      "Time to train HDP model on businesses:  0.03883817195892334 min\n",
      "progress: num of topics:  24\n",
      "Time to train HDP model on businesses:  0.019195361932118734 min\n",
      "progress: num of topics:  27\n",
      "Time to train HDP model on businesses:  0.019262961546579995 min\n",
      "progress: num of topics:  30\n",
      "Time to train HDP model on businesses:  0.02007108529408773 min\n",
      "progress: num of topics:  33\n",
      "Time to train HDP model on businesses:  0.02058444420496623 min\n",
      "progress: num of topics:  36\n",
      "Time to train HDP model on businesses:  0.021619796752929688 min\n",
      "progress: num of topics:  39\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (13,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2ebf2523695f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0meval_top_k_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-2ebf2523695f>\u001b[0m in \u001b[0;36meval_top_k_words\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m41\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_coherence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdp_coherence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hdp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsi_coherence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lsi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                         \u001b[0;34m\"the Matplotlib list!)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1810\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1612\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 231\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (13,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tune hyperparameter top_k_words parameter num_topics\n",
    "def eval_top_k_words():\n",
    "    for fn in ('mesa_5000.csv', 'mesa_7500.csv', 'mesa_10000.csv'):\n",
    "        df = pd.read_csv(fn, index_col=0)\n",
    "        df['tokenized'] = df['tokenized'].apply(eval)\n",
    "        dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "        print('preprocessing ' + fn + ' finished')\n",
    "        _, lsi_coherence = select_num_topics_LSI(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=40)\n",
    "        _, lda_coherence = select_num_topics_LDA(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=40)\n",
    "        _, hdp_coherence = select_num_topics_HDP(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=40)\n",
    "        x = range(4, 41, 4)\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        ax.plot(x, lda_coherence, color='r', label='lda')\n",
    "        ax.plot(x, hdp_coherence, color='g', label='hdp')\n",
    "        ax.plot(x, lsi_coherence, color='b', label='lsi')\n",
    "        ax.legend(loc=\"best\")\n",
    "        ax.set(xlabel='Num of topics', ylabel='Coherence values')\n",
    "        ax.set_title('Topic coherence for different topics for LDA, HDP, and LSI in ' + fn.split('.')[0])\n",
    "        fig.savefig('num_topics_' + fn.split('.')[0] + '.png')\n",
    "        print('computing ' + fn + ' finished')\n",
    "        \n",
    "        \n",
    "eval_top_k_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_name = 'mesa_5000'\n",
    "# df = pd.read_csv(csv_name, index_col=0)\n",
    "# df['tokenized'] = df['tokenized'].apply(eval)\n",
    "# dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "# Set onepass=False, tune power_iters parameter for LSI model\n",
    "def tune_power_iters(dictionary, corpus, num_topics, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_iter in range(3, 31, 3):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    plt.plot(range(3, 31, 3), coherence_values)\n",
    "    plt.title('LSI power_iters parameter experiment')\n",
    "    plt.xlabel(\"Num of iterations\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi_num_iters.png')\n",
    "        \n",
    "    return model_list, coherence_values\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "tune_power_iters(dictionary, corpus, num_topics=12, texts=df['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set onepass=False, tune extra_samples parameter for LSI model\n",
    "def tune_extra_samples(dictionary, corpus, num_topics, num_iter, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_sample in range(100, 300, 30):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter, extra_samples=num_sample)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    plt.plot(range(100, 300, 30), coherence_values)\n",
    "    plt.title('LSI extra_samples parameter experiment')\n",
    "    plt.xlabel(\"Num of extra samples\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi_extra_samples.png')\n",
    "    \n",
    "    return model_list, coherence_values\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "tune_extra_samples(dictionary, corpus, num_topics=12, num_iter=50, texts=df['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "passes = list(range(10, 51, 10))\n",
    "iterations = list(range(100, 410, 60))\n",
    "\n",
    "# Set eval_every to 1 and do grid search of passes and iterations for LDA model\n",
    "def LDA_grid_search(corpus, dictionary, texts, num_topics):\n",
    "    coherence_values = []\n",
    "    pass_iter_pairs = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_pass in passes:\n",
    "        for num_iter in iterations:\n",
    "            pass_iter_pairs.append((num_pass, num_iter))\n",
    "            print(f\"--------------- pass: {num_pass}, iter: {num_iter}--------------\")\n",
    "            model = train_lda(corpus=corpus, id2word=dictionary.id2token, num_topics=num_topics, passes=num_pass,\n",
    "                              iterations=num_iter, eval_every=1, random_state=0)\n",
    "            model_list.append(model)\n",
    "            coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherence_model.get_coherence())\n",
    "            print()\n",
    "            \n",
    "    plt.plot(range(0, len(pass_iter_pairs)), coherence_values)\n",
    "    plt.title('passes and iterations to select best LDA model')\n",
    "    plt.xlabel(\"Index of pass and iteration pairs\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('passes_iters_LDA.png')\n",
    "    \n",
    "    return pass_iter_pairs, model_list, coherence_values\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "pi_pairs, models, coherence_values = LDA_grid_search(corpus=corpus, dictionary=dictionary, texts, num_topics=12, \n",
    "                                                     texts=df['tokenized'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune no_above and no_below parameters in filter_extremes method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search on T, K parameters in HDP model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try two optimized versions of LDA model: ldavowpalwabbit and ldamallet if time allows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Hellinger, Kullbackâ€“Leibler to measure distance metrics for probability distributions after selecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.016919672),\n",
       " (2, 0.086762264),\n",
       " (3, 0.032220233),\n",
       " (4, 0.23659779),\n",
       " (5, 0.022509795),\n",
       " (8, 0.01265313),\n",
       " (9, 0.030311227),\n",
       " (10, 0.17654638),\n",
       " (11, 0.37557247)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.025580758),\n",
       " (2, 0.09432872),\n",
       " (3, 0.04854446),\n",
       " (4, 0.20399308),\n",
       " (5, 0.02056593),\n",
       " (8, 0.026410503),\n",
       " (9, 0.036607433),\n",
       " (10, 0.198711),\n",
       " (11, 0.33592972)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhetpyaHWLUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LSI model on businesses:  0.001632988452911377 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   7.95121635,  101.92274244,   59.16168337, -199.53637389,\n",
       "         -7.92914503,    5.96253872])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_name = 'mesa_5000.csv'\n",
    "query = pd.read_csv(csv_name, index_col=0)\n",
    "query = query.loc[query['name']==\"Alessia's Ristorante Italiano\"]\n",
    "query['tokenized'] = query['tokenized'].apply(eval)\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "lsi_model = train_hdp(corpus=corpus, id2word=dictionary, T=12)\n",
    "lsi_model[corpus][0]\n",
    "query_bow = lsi_model.id2word.doc2bow(query.iloc[0]['tokenized'])\n",
    "lsi_model[query_bow]\n",
    "most_sim_ids = get_most_similar_businesses(query.iloc[0]['tokenized'], corpus, dictionary, lsi_model)\n",
    "tmp = df.iloc[most_sim_ids,:]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train HDP model on businesses:  0.007365183035532633 min\n",
      "model[corpus] shape:  284 2\n",
      "doc dist shape:  (284,)\n",
      "model[corpus] shape:  5 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-595847a9e451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhdp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmost_sim_ids_hdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdp_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_most_similar_businesses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m                                                                \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmost_similar_df_hdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmost_sim_ids_hdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36mget_most_similar_businesses\u001b[0;34m(query_data, corpus, dictionary, model)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mdoc_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doc dist shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mnew_doc_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new doct dist shape before flatten: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_doc_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mnew_doc_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_doc_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36mget_topic_dist\u001b[0;34m(model, corpus)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model[corpus] shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model[corpus] shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model[corpus] shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "csv_name = 'mesa_5000.csv'\n",
    "query = pd.read_csv(csv_name, index_col=0)\n",
    "query = query.loc[query['name']==\"Alessia's Ristorante Italiano\"]\n",
    "query['tokenized'] = query['tokenized'].apply(eval)\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "hdp_model = train_hdp(corpus=corpus, id2word=dictionary, T=12, random_state=0)\n",
    "most_sim_ids_hdp, _, hdp_topic_dist = get_most_similar_businesses(query.iloc[0]['tokenized'], corpus,                                                                dictionary, hdp_model)\n",
    "most_similar_df_hdp = df[df.index.isin(most_sim_ids_hdp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(lsi_topic_dist, open(\"processor.pkl\",\"wb\"))\n",
    "preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "# print the results\n",
    "most_similar_df = df[df.index.isin(most_sim_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zONtAIhpfIV_"
   },
   "outputs": [],
   "source": [
    "# get the ids of the most similar businesses\n",
    "# new_bow = dictionary.doc2bow()\n",
    "# new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "# most_sim_ids = get_most_similar_documents(new_doc_distribution, doc_topic_dist)\n",
    "\n",
    "\n",
    "\n",
    "# print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDBy4J73vH3B"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(doc_topic_dist,open(\"processor.pkl\",\"wb\"))\n",
    "preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "# print the results\n",
    "most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OTkrbs7zvoo"
   },
   "outputs": [],
   "source": [
    "# def process_query(preprocessed, query):\n",
    "#   # SQL to pandas DataFrame w/ query\n",
    "#   query = query.groupby(['name'])['text'].apply(' '.join).reset_index()\n",
    "#   query = query[query['text'].map(type) == str]\n",
    "#   query.dropna(axis=0, inplace=True, subset=['text'])\n",
    "#   query['tokenized'] = query['text'].apply(apply_all)\n",
    "  \n",
    "#   # read the cached pickle\n",
    "#   preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "  \n",
    "#   # get the ids of the most similar businesses\n",
    "#   new_bow = dictionary.doc2bow(query.iloc[0,2])\n",
    "#   new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "#   most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "#   # print the results\n",
    "#   most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "#   print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'y-label'), Text(0.5, 0, 'x-label')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.arange(3, 31, 3)\n",
    "y = np.exp(x)\n",
    "fig1, ax1 = plt.subplots(figsize=(8,5))\n",
    "ax1.plot(x, y, color='r', label='h')\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.set(xlabel='x-label', ylabel='y-label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import is_corpus\n",
    "corpus = [(1, 1.0)]\n",
    "corpus_or_not, corpus = is_corpus(corpus)\n",
    "corpus_or_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FunctionLDA.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
