{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7TpVKeITKFY"
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import os\n",
    "from gensim.matutils import kullback_leibler, jaccard, hellinger, jensen_shannon\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "print('Downloads Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbTRSwxgTvUC"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def initial_clean(text):\n",
    "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = text.lower() # lower case the text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "def pos(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "informative_pos = ('JJ','VB', 'NN','RBS','VBP','IN','RBR','JJR','JJS','PDT','RP','UH','FW','NNS','VBN','VBG')\n",
    "\n",
    "def remove_uninformative_pos(text):\n",
    "    tagged_words = nltk.pos_tag(text)\n",
    "    return [word for word, tag in tagged_words if tag in informative_pos]\n",
    "  \n",
    "clutter = ['food','place','good','order','great','like',\n",
    "           'service','time','go','ordered','get','love',\n",
    "           'best','come','eat','dont','tried','try','ask',\n",
    "           'nice','restaurant','ive','im','didnt']\n",
    "\n",
    "def remove_garbage(text):\n",
    "    return [word for word in text if word not in clutter]\n",
    "\n",
    "def stem_words(text):\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
    "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    return stem_words(remove_garbage(remove_uninformative_pos(remove_stop_words(initial_clean(text)))))\n",
    "\n",
    "def get_top_k_words(df, k = 10000):\n",
    "    # first get a list of all words\n",
    "    all_words = [word for item in list(df['tokenized']) for word in item]\n",
    "    \n",
    "    # use nltk fdist to get a frequency distribution of all words\n",
    "    fdist = FreqDist(all_words)\n",
    "    \n",
    "    # define a function only to keep words in the top k words\n",
    "    top_k_words, _ = zip(*fdist.most_common(k))\n",
    "    top_k_words = set(top_k_words)\n",
    "    \n",
    "    return top_k_words\n",
    "\n",
    "def keep_top_k_words(text, *top_k_words):\n",
    "    return [word for word in text if word in top_k_words]\n",
    "\n",
    "def transform_dataset(df):\n",
    "    # format the columns\n",
    "    df = df.groupby(['business_id', 'name'])['text'].apply(' '.join).reset_index()\n",
    "    df = df[df['text'].map(type) == str]\n",
    "    df.dropna(axis=0, inplace=True, subset=['text'])\n",
    "    return df\n",
    "\n",
    "def gen_tokenized_column(df):\n",
    "    # preprocess the text and business name and create new column \"tokenized\"\n",
    "    df['tokenized'] = df['text'].apply(apply_all)\n",
    "    top_k_words = get_top_k_words(df)\n",
    "    df['tokenized'] = df['tokenized'].apply(keep_top_k_words, args=(top_k_words))\n",
    "    return df\n",
    "\n",
    "def preprocess_dataset(df):\n",
    "    t1 = time.time()\n",
    "    preprocessed_df = gen_tokenized_column(transform_dataset(df))\n",
    "    t2 = time.time()\n",
    "    print(\"Time to clean and tokenize\", len(df), \"businesses' reviews:\", (t2-t1)/60, \"min\")\n",
    "    return preprocessed_df\n",
    "    \n",
    "def get_coherence_score(model, texts, dictionary):\n",
    "    coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "def get_dictionary_corpus(data, no_below=20, no_above=0.5):\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in data]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def get_perplexity(model, corpus):\n",
    "    # a measure of how good the model is; lower the better\n",
    "    return lda_model.log_perplexity(corpus)\n",
    "\n",
    "def train_lda(corpus, id2word, chunksize=2000, num_topics=12, alpha='auto', eta='auto', passes=1, iterations=50,\n",
    "              minimum_probability=0.01, eval_every=10, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, alpha=alpha, eta=eta, \n",
    "                   chunksize=chunksize, minimum_probability=minimum_probability, passes=passes, \n",
    "                   iterations=iterations, eval_every=eval_every, random_state=random_state)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lda\n",
    "\n",
    "def train_hdp(corpus, id2word, chunksize=2000, T=150, random_state=None):\n",
    "    \"\"\"\n",
    "    This function trains the hdp model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    hdp = HdpModel(corpus=corpus, id2word=id2word, T=T, chunksize=chunksize, random_state=random_state)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train HDP model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return hdp\n",
    "\n",
    "def train_lsi(corpus, id2word, num_topics=12, chunksize=2000, onepass=True, power_iters=2, extra_samples=100):\n",
    "    \"\"\"\n",
    "    This function trains the lsi model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    \"\"\"\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lsi = LsiModel(corpus=corpus, num_topics=num_topics, id2word=id2word, chunksize=chunksize)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LSI model on businesses: \", (t2-t1)/60, \"min\")\n",
    "    \n",
    "    return lsi\n",
    "\n",
    "def get_most_similar_documents(query, corpus, dictionary, k=10):\n",
    "    distances = []\n",
    "    for c in corpus:\n",
    "        distances.append(kullback_leibler(query, c, num_features=len(dictionary)))\n",
    "    \n",
    "    indices = np.array(distances).argsort()[:k]\n",
    "    return indices\n",
    "\n",
    "def get_topic_dist(model, corpus):\n",
    "    doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in model[corpus]])\n",
    "    return doc_topic_dist\n",
    "\n",
    "def get_most_similar_businesses(query_data, corpus, dictionary, model):\n",
    "    query_bow = dictionary.doc2bow(query_data)\n",
    "    most_sim_ids = get_most_similar_documents(model[query_bow], model[corpus], dictionary)\n",
    "    return most_sim_ids \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LDA\n",
    "def select_num_topics_LDA(dictionary, corpus, texts, end, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lda(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_num_topics_HDP(dictionary, corpus, texts, end, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the HDP model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, end + 1, step): \n",
    "        model = train_hdp(corpus=corpus, id2word=dictionary, T=num_topics, random_state=0)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "            \n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "    \n",
    "    return model, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of topics by computing coherence score for LSI\n",
    "def select_num_topics_LSI(dictionary, corpus, texts, end, start=3, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    end : Max num of topics\n",
    "    start: Min num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LSI topic models\n",
    "    coherence_values : Coherence values corresponding to the Lsi model with respective number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "\n",
    "    for num_topics in range(start, end + 1, step):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        print('progress: num of topics: ', num_topics)\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing mesa_5000.csv finished\n",
      "Time to train LSI model on businesses:  0.008956185976664225 min\n",
      "progress: num of topics:  3\n",
      "Time to train LSI model on businesses:  0.004711604118347168 min\n",
      "progress: num of topics:  6\n",
      "Time to train LSI model on businesses:  0.004893116156260173 min\n",
      "progress: num of topics:  9\n",
      "Time to train LSI model on businesses:  0.004853351910909017 min\n",
      "progress: num of topics:  12\n",
      "Time to train LSI model on businesses:  0.004771041870117188 min\n",
      "progress: num of topics:  15\n",
      "Time to train LSI model on businesses:  0.004885625839233398 min\n",
      "progress: num of topics:  18\n",
      "Time to train LSI model on businesses:  0.005016847451527914 min\n",
      "progress: num of topics:  21\n",
      "Time to train LSI model on businesses:  0.005815680821736654 min\n",
      "progress: num of topics:  24\n",
      "Time to train LSI model on businesses:  0.005471324920654297 min\n",
      "progress: num of topics:  27\n",
      "Time to train LSI model on businesses:  0.005101430416107178 min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a01023927fc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0meval_top_k_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-a01023927fc3>\u001b[0m in \u001b[0;36meval_top_k_words\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dictionary_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'preprocessing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsi_coherence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_num_topics_LSI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlda_coherence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_num_topics_LDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdp_coherence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_num_topics_HDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-bc425b36251f>\u001b[0m in \u001b[0;36mselect_num_topics_LSI\u001b[0;34m(dictionary, corpus, texts, end, start, step)\u001b[0m\n\u001b[1;32m     32\u001b[0m         coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n\u001b[1;32m     33\u001b[0m                                          coherence='c_v')\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mcoherence_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'progress: num of topics: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \"\"\"\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mconfirmed_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence_per_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfirmed_measures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msegmented_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mestimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyed_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/topic_coherence/probability_estimation.py\u001b[0m in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using %s to estimate probabilities from sliding windows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36maccumulate\u001b[0;34m(self, texts, window_size)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0minterrupted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0maccumulators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterrupted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_accumulators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36mterminate_workers\u001b[0;34m(self, input_q, output_q, workers, interrupted)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minterrupted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                 \u001b[0minput_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0maccumulators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/queues.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mFull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process AccumulatingWorker-178:\n",
      "Process AccumulatingWorker-180:\n",
      "Process AccumulatingWorker-179:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/util.py\", line 314, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/util.py\", line 314, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/util.py\", line 314, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/util.py\", line 254, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/util.py\", line 254, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/util.py\", line 254, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/util.py\", line 186, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 198, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 198, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1054, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 198, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1054, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1054, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 1070, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Tune hyperparameter top_k_words parameter num_topics\n",
    "def eval_top_k_words():\n",
    "    for fn in ('mesa_5000.csv', 'mesa_7500.csv', 'mesa_10000.csv'):\n",
    "        df = pd.read_csv(fn, index_col=0)\n",
    "        df['tokenized'] = df['tokenized'].apply(eval)\n",
    "        dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "        print('preprocessing ' + fn + ' finished')\n",
    "        _, lsi_coherence = select_num_topics_LSI(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=40)\n",
    "        _, lda_coherence = select_num_topics_LDA(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=40)\n",
    "        _, hdp_coherence = select_num_topics_HDP(dictionary=dictionary, corpus=corpus, texts=df['tokenized'], end=40)\n",
    "        x = range(4, 41, 4)\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        ax.plot(x, lda_coherence, color='r', label='lda')\n",
    "        ax.plot(x, hdp_coherence, color='g', label='hdp')\n",
    "        ax.plot(x, lsi_coherence, color='b', label='lsi')\n",
    "        ax.legend(loc=\"best\")\n",
    "        ax.set(xlabel='Num of topics', ylabel='Coherence values')\n",
    "        ax.set_title('Topic coherence for different topics for LDA, HDP, and LSI in ' + fn.split('.')[0])\n",
    "        fig.savefig('num_topics_' + fn.split('.')[0] + '.png')\n",
    "        print('computing ' + fn + ' finished')\n",
    "        \n",
    "        \n",
    "eval_top_k_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_name = 'mesa_5000'\n",
    "# df = pd.read_csv(csv_name, index_col=0)\n",
    "# df['tokenized'] = df['tokenized'].apply(eval)\n",
    "# dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "# Set onepass=False, tune power_iters parameter for LSI model\n",
    "def tune_power_iters(dictionary, corpus, num_topics, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_iter in range(3, 31, 3):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    plt.plot(range(3, 31, 3), coherence_values)\n",
    "    plt.title('LSI power_iters parameter experiment')\n",
    "    plt.xlabel(\"Num of iterations\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi_num_iters.png')\n",
    "        \n",
    "    return model_list, coherence_values\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "tune_power_iters(dictionary, corpus, num_topics=12, texts=df['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set onepass=False, tune extra_samples parameter for LSI model\n",
    "def tune_extra_samples(dictionary, corpus, num_topics, num_iter, texts):\n",
    "    \n",
    "    model_list = []\n",
    "    coherence_values = []\n",
    "    \n",
    "    for num_sample in range(100, 300, 30):\n",
    "        model = train_lsi(corpus=corpus, id2word=dictionary, num_topics=num_topics, onepass=False, \n",
    "                          power_iters=num_iter, extra_samples=num_sample)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        topics = []\n",
    "        for topic_id, topic in model.show_topics(num_topics=num_topics, formatted=False):\n",
    "            topic = [word for word, _ in topic]\n",
    "            topics.append(topic)\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topics, model=model, texts=texts, dictionary=dictionary, \n",
    "                                         coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    \n",
    "    plt.plot(range(100, 300, 30), coherence_values)\n",
    "    plt.title('LSI extra_samples parameter experiment')\n",
    "    plt.xlabel(\"Num of extra samples\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('lsi_extra_samples.png')\n",
    "    \n",
    "    return model_list, coherence_values\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "tune_extra_samples(dictionary, corpus, num_topics=12, num_iter=50, texts=df['tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "passes = list(range(10, 51, 10))\n",
    "iterations = list(range(100, 410, 60))\n",
    "\n",
    "# Set eval_every to 1 and do grid search of passes and iterations for LDA model\n",
    "def LDA_grid_search(corpus, dictionary, texts, num_topics):\n",
    "    coherence_values = []\n",
    "    pass_iter_pairs = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_pass in passes:\n",
    "        for num_iter in iterations:\n",
    "            pass_iter_pairs.append((num_pass, num_iter))\n",
    "            print(f\"--------------- pass: {num_pass}, iter: {num_iter}--------------\")\n",
    "            model = train_lda(corpus=corpus, id2word=dictionary.id2token, num_topics=num_topics, passes=num_pass,\n",
    "                              iterations=num_iter, eval_every=1, random_state=0)\n",
    "            model_list.append(model)\n",
    "            coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_values.append(coherence_model.get_coherence())\n",
    "            print()\n",
    "            \n",
    "    plt.plot(range(0, len(pass_iter_pairs)), coherence_values)\n",
    "    plt.title('passes and iterations to select best LDA model')\n",
    "    plt.xlabel(\"Index of pass and iteration pairs\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('passes_iters_LDA.png')\n",
    "    \n",
    "    return pass_iter_pairs, model_list, coherence_values\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "pi_pairs, models, coherence_values = LDA_grid_search(corpus=corpus, dictionary=dictionary, texts, num_topics=12, \n",
    "                                                     texts=df['tokenized'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune no_above and no_below parameters in filter_extremes method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search on T, K parameters in HDP model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try two optimized versions of LDA model: ldavowpalwabbit and ldamallet if time allows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Hellinger, Kullbackâ€“Leibler to measure distance metrics for probability distributions after selecting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.016919672),\n",
       " (2, 0.086762264),\n",
       " (3, 0.032220233),\n",
       " (4, 0.23659779),\n",
       " (5, 0.022509795),\n",
       " (8, 0.01265313),\n",
       " (9, 0.030311227),\n",
       " (10, 0.17654638),\n",
       " (11, 0.37557247)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.025580758),\n",
       " (2, 0.09432872),\n",
       " (3, 0.04854446),\n",
       " (4, 0.20399308),\n",
       " (5, 0.02056593),\n",
       " (8, 0.026410503),\n",
       " (9, 0.036607433),\n",
       " (10, 0.198711),\n",
       " (11, 0.33592972)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qhetpyaHWLUm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train LSI model on businesses:  0.001632988452911377 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   7.95121635,  101.92274244,   59.16168337, -199.53637389,\n",
       "         -7.92914503,    5.96253872])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_name = 'mesa_5000.csv'\n",
    "query = pd.read_csv(csv_name, index_col=0)\n",
    "query = query.loc[query['name']==\"Alessia's Ristorante Italiano\"]\n",
    "query['tokenized'] = query['tokenized'].apply(eval)\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "lsi_model = train_hdp(corpus=corpus, id2word=dictionary, T=12)\n",
    "lsi_model[corpus][0]\n",
    "query_bow = lsi_model.id2word.doc2bow(query.iloc[0]['tokenized'])\n",
    "\n",
    "most_sim_ids = get_most_similar_businesses(query.iloc[0]['tokenized'], corpus, dictionary, lsi_model)\n",
    "tmp = df.iloc[most_sim_ids,:]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train HDP model on businesses:  0.007365183035532633 min\n",
      "model[corpus] shape:  284 2\n",
      "doc dist shape:  (284,)\n",
      "model[corpus] shape:  5 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-595847a9e451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhdp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_hdp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmost_sim_ids_hdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdp_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_most_similar_businesses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m                                                                \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmost_similar_df_hdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmost_sim_ids_hdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36mget_most_similar_businesses\u001b[0;34m(query_data, corpus, dictionary, model)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mdoc_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doc dist shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mnew_doc_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new doct dist shape before flatten: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_doc_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mnew_doc_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_doc_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36mget_topic_dist\u001b[0;34m(model, corpus)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model[corpus] shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model[corpus] shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-7684b66e21ee>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_topic_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model[corpus] shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mdoc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_dist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "csv_name = 'mesa_5000.csv'\n",
    "query = pd.read_csv(csv_name, index_col=0)\n",
    "query = query.loc[query['name']==\"Alessia's Ristorante Italiano\"]\n",
    "query['tokenized'] = query['tokenized'].apply(eval)\n",
    "\n",
    "df = pd.read_csv('mesa_5000.csv', index_col=0)\n",
    "df['tokenized'] = df['tokenized'].apply(eval)\n",
    "dictionary, corpus = get_dictionary_corpus(df['tokenized'])\n",
    "\n",
    "hdp_model = train_hdp(corpus=corpus, id2word=dictionary, T=12, random_state=0)\n",
    "most_sim_ids_hdp, _, hdp_topic_dist = get_most_similar_businesses(query.iloc[0]['tokenized'], corpus,                                                                dictionary, hdp_model)\n",
    "most_similar_df_hdp = df[df.index.isin(most_sim_ids_hdp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(lsi_topic_dist, open(\"processor.pkl\",\"wb\"))\n",
    "preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "# print the results\n",
    "most_similar_df = df[df.index.isin(most_sim_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zONtAIhpfIV_"
   },
   "outputs": [],
   "source": [
    "# get the ids of the most similar businesses\n",
    "# new_bow = dictionary.doc2bow()\n",
    "# new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "# most_sim_ids = get_most_similar_documents(new_doc_distribution, doc_topic_dist)\n",
    "\n",
    "\n",
    "\n",
    "# print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDBy4J73vH3B"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(doc_topic_dist,open(\"processor.pkl\",\"wb\"))\n",
    "preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "# print the results\n",
    "most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OTkrbs7zvoo"
   },
   "outputs": [],
   "source": [
    "# def process_query(preprocessed, query):\n",
    "#   # SQL to pandas DataFrame w/ query\n",
    "#   query = query.groupby(['name'])['text'].apply(' '.join).reset_index()\n",
    "#   query = query[query['text'].map(type) == str]\n",
    "#   query.dropna(axis=0, inplace=True, subset=['text'])\n",
    "#   query['tokenized'] = query['text'].apply(apply_all)\n",
    "  \n",
    "#   # read the cached pickle\n",
    "#   preprocessed = pickle.load(open(\"processor.pkl\",\"rb\"))\n",
    "  \n",
    "#   # get the ids of the most similar businesses\n",
    "#   new_bow = dictionary.doc2bow(query.iloc[0,2])\n",
    "#   new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])\n",
    "#   most_sim_ids = get_most_similar_documents(new_doc_distribution,preprocessed)\n",
    "\n",
    "#   # print the results\n",
    "#   most_similar_df = df[df.index.isin(most_sim_ids)]\n",
    "#   print('Similar to \"{}\": \\n{}'.format(query['name'][0], most_similar_df['name'].reset_index(drop=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'y-label'), Text(0.5, 0, 'x-label')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.arange(3, 31, 3)\n",
    "y = np.exp(x)\n",
    "fig1, ax1 = plt.subplots(figsize=(8,5))\n",
    "ax1.plot(x, y, color='r', label='h')\n",
    "ax1.legend(loc=\"best\")\n",
    "ax1.set(xlabel='x-label', ylabel='y-label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import is_corpus\n",
    "corpus = [(1, 1.0)]\n",
    "corpus_or_not, corpus = is_corpus(corpus)\n",
    "corpus_or_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FunctionLDA.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
